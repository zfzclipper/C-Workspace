*Tail Recursion
(1)The use of ONLY ONE recursive call at the VERY END of a function implementation.
	(a)When the function call is made, there are NO statements left to be executed by the function;
	(b)The recursive call is not only the last statement, but there are NO earlier recursive calls, direct or indirect;
(2)Tail recursion could be replaced by one loop.

*Sorting
(1)It is often necessary to sort data before processing.
(2)The first step is to choose the criteria that will be used to order data.
(3)Two critical properties of sorting algorithms:
	(a)Number of comparisons;
	(b)Number of data movements;

--------------
| Quick Sort |
--------------
Divide-and-Conquer Recursive Algorithm;

Time Complexity: 
(1)Best Case Performance (balanced partition):		O(n*log(n));
(2)Worst Case Performance (unbalanced partition):	O(n^2);

The running time depends on whether the partitioning is balanced or unbalanced, which in turn depends on which "pivots" are used for partitioning.
(1)If the partitioning is balanced, the algorithm runs asymptotically as fast as merge sort O(n*log(n));
	e.g.: The O(n*log(n)) performance is achieved by equally balancing the two sides of the partition at every level of the recursion,
(2)If the partitioning is unbalanced, it can run asymptotically as slowly as insertion sort O(n^2);
	e.g,: The O(n^2) running time occurs when the input array is already completely sorted.
(3)*The average-case running time of quicksort is much closer to the best case than to the worst case.
(4)The running time is O(n*lg(n) whenever the split has constant proportionality.	
	Any split of constant proportionality yields a recursion tree of depth O(lg(n)), where the cost at each level is O(n).
(5)The running time when levels alternate between good and bad splits is still O(n*lg(n)), but with a slightly larger hidden constant c;

--------------------
Randomized Quicksort
--------------------
*To get close to the best-case running time, the pivot should be chosen in such a way that the input sequence is almost "equally partitioned".
Random Sampling: Randomly choose the pivot element, in order to make the "split" of the input array reasonably "well-balanced" on average.

----------------
In-Place Sorting
----------------
It modifies the input sequence using element swapping and does not explicitly create subsequences.
It reduces the running time caused by the creation of new sequences and the movement of elements between them by a constant factor.

*We can achieve quicksort’s fast running time on almost all inputs, with heapsort’s O(N log N) worst-case running time.




[+]Unlike merge-sort, the height of the quick-sort tree associated with an execution of quick-sort is linear in the worst case.
	if the sequence consists of n distinct elements and is already sorted

Divide the whole sequence into sub-sequences, recur to sort each subsequence, and then combine the sorted subsequences by a simple concatenation.
[1]Divide: Remove all the elements from S and put them into three sequences:
		(1)L: storing the elements in S less than x;
		(2)E: storing the elements in S equal to x;
		(3)G: storing the elements in S greater than x;
[2]Recur: Recursively sort sequences L and G.
[3]Conquer: Put back the elements into S in order by first inserting the elements of L, then those of E, and finally those of G. 



--------------
| Merge Sort |
--------------
Divide-and-Conquer Recursive Algorithm;
Time Complexity: O(n*log(n));

Implementation:
[+]List-based Recursive Algorithm
[+]Vector-based Recursive Algorithm
[ ]Vector-based Non-recursive Algorithm

Divide-and-Conquer Approach:
(1)Divide: Break the problem into several subproblems that are similar to the original problem but smaller in size;
(2)Conquer: Solve the subproblems recursively;
(3)Combine: Combine these solutions to create a solution to the original problem;


------------------
| Insertion Sort |
------------------
Time Complexity: O(n^2);
Insertion sort consists of N − 1 passes. 
Invariant: at each pass p <- [1, N-1], elements in positions 0 through p − 1 are already in sorted order.

[+]It sorts the array only when it is really necessary.
	If the array is already in order: only the variable tmp is initialized, and the value stored in it is moved back to the same position.
[-]Insertion is not localized: it may require moving a significant number of elements.
	If an item is being inserted, all elements greater than the one being inserted have to be moved. 
[-]The number of movements and comparisons for a randomly ordered array is closer to the worst case O(n^2).


------------------
| Selection Sort |
------------------
Time Complexity: O(n^2);
Invariant: at each pass, the smallest element in the unsorted sub-array is selected and swapped with the first position in unsorted sub-array.
[+]The required number of assignments is the best compared with any other algorithm.


---------------
| Bubble Sort |
---------------
Time Complexity: O(n^2);
[-]In the average case, approximately twice as many comparisons, and the same number of moves as insertion sort
			as many comparisons, and n times more moves than selection sort.
[-]Insertion sort is twice as fast as bubble sort.

